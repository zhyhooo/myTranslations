# [梯度提升方法概述和演示](https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)

翻译：KK4SBB

![GB](https://arogozhnikov.github.io/images/gbdt_attractive_picture.png)

梯度提升（Gradient boosting）是上世纪90年代后期提出的一种机器学习算法，如今依然十分流行。它在许多商业产品（还有学术项目）上取得了杰出的成绩。

本文用一些交互式的可视化工具来展示梯度提升算法的工作原理。

## 决策树的可视化

[]

我们以2维回归问题为例，探讨决策树是如何复原函数<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi<mn>2</mn></msub><mo stretchy="false">)</mo></math>的。尝试改变树的深度，然后观察树的构建过程。

## 解释：决策树是如何构建的？

首先，我们先来复习一下决策树的工作流程。一棵决策树就是一个简单的分类器，将特征空间按照特征值（比如，x2 < 2.4 ）分割成多个区域。每个区域是一块矩形空间。每块区域内的预测结果都是固定的。

梯度提升是基于回归树的（即使是解决一个分类问题），它的目标是最小化均方误差（MSE）。给叶子节点选择一个预测值的过程非常简单：为了最小化MSE，我们应该在叶节点选择所有样本目标值的均值。决策树的构建是由根节点开始贪婪地发展：每个叶节点处的分裂都是以最小化MSE为目标。

这种贪婪的特性使得树的构建非常快，但是最终得到的并不是最优的结果。构建最优的决策树是一个非常难的问题（事实上是NP完全问题）。

与此同时，这些构建的树效果非常好（比人们通常认为的更好），而且也适用于我们下一步的任务。

## 梯度提升的可视化

下面这个示例演示了100棵决策树的融合结果。

【】

不算太坏，对吧？如我们所见，梯度提升可以通过组合多棵深度较浅的树得到平滑的预测结果。

## 解释：什么是梯度提升

首先，梯度提升是一种融合技术，换句话说就是预测结果是多个简单预测器的融合。尽管理论上这种框架下可能产生多种预测器的组合，实践中我们往往选用GBDT —— 梯度提升的决策树模型。这正是我在demo里用到的例子，但是理论上其它任何的预测器都能替代决策树。

梯度提升的目标就是创建（或者说训练）多棵决策树的融合，假设我们已经掌握了如何训练单棵决策树。我们把这样的技术称为**提升（boosting）**，因为我们希望多棵树融合的效果能好于单棵树。

## 融合的方法

接下来就是最重要的部分。梯度提升是**一颗接一颗**地完成融合树的构建，然后对每棵树的预测结果**求和**。

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>D</mi>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <msub>
    <mi>d</mi>
    <mtext>tree 1</mtext>
  </msub>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <msub>
    <mi>d</mi>
    <mtext>tree 2</mtext>
  </msub>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mo>.</mo>
  <mo>.</mo>
  <mo>.</mo>
</math>

接下去的一棵决策树通过**重建残差**来弥补目标函数<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo></math>与当前融合树的差值。

举个例子，如果融合模型有三棵树组成，那么融合模型的输出结果为：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>D</mi>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <msub>
    <mi>d</mi>
    <mtext>tree 1</mtext>
  </msub>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <msub>
    <mi>d</mi>
    <mtext>tree 2</mtext>
  </msub>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <msub>
    <mi>d</mi>
    <mtext>tree 3</mtext>
  </msub>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
</math>

下一棵树（即第四棵树）应该补充现有的结果，使得融合模型的训练误差最小化。

理想情况下，我们希望看到：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>D</mi>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <msub>
    <mi>d</mi>
    <mtext>tree 4</mtext>
  </msub>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>

为了更加拟合目标值我们训练一棵树来重建目标函数与当前融合模型预测结果的差值，即所谓的**残差**：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>R</mi>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>&#x2212;<!-- − --></mo>
  <mi>D</mi>
  <mo stretchy="false">(</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>

你注意到了吗？如果决策树能够完全重建<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo></math>，那融合的预测结果可以完全消除误差（再加上最新训练得到的一棵树之后）！然而，实际情况并非如此，所以我们需要不停地迭代构建融合模型。

【】

请看右边的图示。注意看经过多棵树的迭代之后，纵轴(y)方向上的值是如何减小的（残差变得越来越小）。

动手调试这个残差示例之后，你也许会发现一些有趣的内容：

- 当一棵决策树加入融合模型之前，它们的预测结果与某些因子相乘
- 这个因子（即η或是学习率）是梯度提升的一个重要参数（此示例中η=0.3）
- 如果我们将决策树的数量设置为10棵，改变树的深度：我们会发现随着树的深度增加，残差项逐渐减小，噪音却越来越大
- 不连续点（‘尖峰’）往往出现在决策树的分裂点附近
- 学习率越大，单棵决策树跨越的‘阶梯’越大，不连续区域也越大
- 实际应用中，GBDT的学习率设置的较小（0.01<η<0.1），通过增加树的数量来提高效果


